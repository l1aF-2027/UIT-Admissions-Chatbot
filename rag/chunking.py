import re
import os
import json
import hashlib
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
import nltk
from nltk.tokenize import sent_tokenize
import unicodedata

# Táº¡o thÆ° má»¥c Ä‘áº§u ra
folder_dir = "chunked_json"
data_dir = "markdown_data"
os.makedirs(folder_dir, exist_ok=True)

# CÃ i Ä‘áº·t NLTK náº¿u cáº§n
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# CÃ¡c tá»« khÃ³a quan trá»ng cho viá»‡c phÃ¢n loáº¡i
TUYEN_SINH_KEYWORDS = [
    "tuyá»ƒn sinh", "Ä‘á»£t", "phÆ°Æ¡ng thá»©c", "Ä‘Äƒng kÃ½", "há»“ sÆ¡", "xÃ©t tuyá»ƒn", 
    "nguyá»‡n vá»ng", "thá»i gian", "chá»‰ tiÃªu", "Ä‘iá»u kiá»‡n", "quy Ä‘á»‹nh", 
    "thÃ´ng bÃ¡o", "hÆ°á»›ng dáº«n", "Ä‘áº¡i há»c", "há»c phÃ­"
]

KET_QUA_KEYWORDS = [
    "káº¿t quáº£", "Ä‘iá»ƒm", "xÃ©t tuyá»ƒn", "trÃºng tuyá»ƒn", "Ä‘iá»ƒm chuáº©n", 
    "danh sÃ¡ch", "thÃ­ sinh", "Ä‘áº­u", "Ä‘iá»ƒm cá»™ng", "Ä‘iá»ƒm Æ°u tiÃªn"
]

NGANH_KEYWORDS = [
    "tá»•ng quan", "ngÃ nh", "khoa", "chuyÃªn ngÃ nh", "chÆ°Æ¡ng trÃ¬nh", 
    "Ä‘Ã o táº¡o", "cÆ¡ há»™i viá»‡c lÃ m", "báº±ng cáº¥p", "giáº£ng viÃªn", "tÃ­n chá»‰"
]

def normalize_text(text):
    """Chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t."""
    if not text:
        return ""
    # Loáº¡i bá» dáº¥u cÃ¢u vÃ  chuyá»ƒn vá» lowercase
    text = unicodedata.normalize('NFC', text)
    text = text.lower().strip()
    return text

def extract_year(text):
    """TrÃ­ch xuáº¥t nÄƒm tá»« ná»™i dung vá»›i nhiá»u máº«u khÃ¡c nhau."""
    # TÃ¬m nÄƒm vá»›i Ä‘á»‹nh dáº¡ng cá»¥ thá»ƒ nhÆ° 2023-2024, nÄƒm 2023, etc.
    year_patterns = [
        r'(20\d{2})\s*-\s*(20\d{2})',  # 2023-2024
        r'nÄƒm\s+(20\d{2})',            # nÄƒm 2023
        r'khÃ³a\s+(20\d{2})',           # khÃ³a 2023
        r'k(20\d{2})',                 # k2023
        r'(20\d{2})[^\d]'              # 2023 (tá»•ng quÃ¡t)
    ]
    
    for pattern in year_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            # Tráº£ vá» nÄƒm Ä‘áº§y Ä‘á»§ hoáº·c nÄƒm Ä‘áº§u tiÃªn náº¿u cÃ³ dáº£i nÄƒm
            if len(match.groups()) > 1 and match.group(2):
                return f"{match.group(1)}-{match.group(2)}"
            return match.group(1)
    
    return None

def extract_department_and_major(text):
    """TrÃ­ch xuáº¥t thÃ´ng tin vá» khoa vÃ  ngÃ nh tá»« vÄƒn báº£n."""
    department_info = {
        "department": None,
        "major": None
    }
    
    # Máº«u tÃ¬m kiáº¿m cho khoa/ngÃ nh
    patterns = [
        # Khoa/ngÃ nh cá»¥ thá»ƒ
        r'khoa\s+([^,.;:]+)',
        r'ngÃ nh\s+([^,.;:]+)',
        r'chuyÃªn\s+ngÃ nh\s+([^,.;:]+)',
        # TÃªn ngÃ nh Ä‘áº·c biá»‡t cá»§a UIT
        r'(cÃ´ng\s+nghá»‡\s+thÃ´ng\s+tin)',
        r'(ká»¹\s+thuáº­t\s+pháº§n\s+má»m)',
        r'(khoa\s+há»c\s+mÃ¡y\s+tÃ­nh)',
        r'(há»‡\s+thá»‘ng\s+thÃ´ng\s+tin)',
        r'(ká»¹\s+thuáº­t\s+mÃ¡y\s+tÃ­nh)',
        r'(an\s+toÃ n\s+thÃ´ng\s+tin)',
        r'(thÆ°Æ¡ng\s+máº¡i\s+Ä‘iá»‡n\s+tá»­)',
        r'(trÃ­\s+tuá»‡\s+nhÃ¢n\s+táº¡o)',
    ]
    
    normalized_text = normalize_text(text)
    
    for pattern in patterns:
        match = re.search(pattern, normalized_text, re.IGNORECASE)
        if match:
            extracted = match.group(1).strip()
            # PhÃ¢n biá»‡t khoa vÃ  ngÃ nh
            if "khoa" in pattern:
                department_info["department"] = extracted
            else:
                department_info["major"] = extracted
    
    return department_info

def detect_field(title, content):
    """PhÃ¡t hiá»‡n lÄ©nh vá»±c chÃ­nh cá»§a vÄƒn báº£n."""
    lowered_text = normalize_text(title + " " + content[:500])  # Chá»‰ sá»­ dá»¥ng 500 kÃ½ tá»± Ä‘áº§u tiÃªn
    
    # TÃ­nh Ä‘iá»ƒm cho má»—i lÄ©nh vá»±c
    tuyen_sinh_score = sum(3 if keyword in lowered_text else 0 for keyword in TUYEN_SINH_KEYWORDS)
    ket_qua_score = sum(3 if keyword in lowered_text else 0 for keyword in KET_QUA_KEYWORDS)
    nganh_score = sum(3 if keyword in lowered_text else 0 for keyword in NGANH_KEYWORDS)
    
    # TÄƒng Ä‘iá»ƒm náº¿u cÃ³ tá»« khÃ³a trong tiÃªu Ä‘á»
    lowered_title = normalize_text(title)
    tuyen_sinh_score += sum(5 if keyword in lowered_title else 0 for keyword in TUYEN_SINH_KEYWORDS)
    ket_qua_score += sum(5 if keyword in lowered_title else 0 for keyword in KET_QUA_KEYWORDS)
    nganh_score += sum(5 if keyword in lowered_title else 0 for keyword in NGANH_KEYWORDS)
    
    # XÃ¡c Ä‘á»‹nh lÄ©nh vá»±c dá»±a trÃªn Ä‘iá»ƒm
    scores = {
        "tuyen_sinh": tuyen_sinh_score,
        "ket_qua": ket_qua_score,
        "nganh": nganh_score
    }
    
    field = max(scores, key=scores.get)
    max_score = scores[field]
    
    # Náº¿u Ä‘iá»ƒm quÃ¡ tháº¥p, coi lÃ  "khÃ¡c"
    if max_score < 5:
        return "khac"
    
    return field

def chunk_content_by_semantics(content, max_tokens=300):
    """Chia ná»™i dung thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ tÃ­nh ngá»¯ nghÄ©a, Ä‘áº£m báº£o khÃ´ng quÃ¡ max_tokens."""
    # Chia vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
    try:
        sentences = sent_tokenize(content)
    except:
        # Fallback náº¿u NLTK gáº·p lá»—i
        sentences = re.split(r'[.!?]', content)
        sentences = [s.strip() + '.' for s in sentences if s.strip()]
    
    chunks = []
    current_chunk = []
    current_token_count = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Æ¯á»›c tÃ­nh sá»‘ token cá»§a cÃ¢u
        tokens_in_sentence = len(sentence.split())
        
        # Náº¿u cÃ¢u Ä‘Æ¡n láº» quÃ¡ dÃ i, cÃ³ thá»ƒ chia nhá» hÆ¡n
        if tokens_in_sentence > max_tokens:
            # Äáº£m báº£o cÃ¢u hiá»‡n táº¡i Ä‘Æ°á»£c thÃªm vÃ o káº¿t quáº£
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_token_count = 0
            
            # Chia cÃ¢u dÃ i thÃ nh cÃ¡c Ä‘oáº¡n nhá» hÆ¡n
            words = sentence.split()
            temp_chunk = []
            for word in words:
                if len(temp_chunk) + 1 > max_tokens:
                    chunks.append(" ".join(temp_chunk))
                    temp_chunk = [word]
                else:
                    temp_chunk.append(word)
            
            if temp_chunk:
                chunks.append(" ".join(temp_chunk))
            continue
        
        # Kiá»ƒm tra náº¿u thÃªm cÃ¢u má»›i vÆ°á»£t quÃ¡ giá»›i háº¡n token
        if current_token_count + tokens_in_sentence > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_token_count = tokens_in_sentence
        else:
            current_chunk.append(sentence)
            current_token_count += tokens_in_sentence
    
    # Xá»­ lÃ½ chunk cuá»‘i cÃ¹ng
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return chunks

def extract_metadata(content, title):
    """TrÃ­ch xuáº¥t metadata tá»« ná»™i dung vÃ  tiÃªu Ä‘á»."""
    metadata = {}
    
    # TrÃ­ch xuáº¥t nÄƒm
    year = extract_year(content) or extract_year(title)
    if year:
        metadata["year"] = year
    
    # TrÃ­ch xuáº¥t thÃ´ng tin khoa/ngÃ nh
    dept_info = extract_department_and_major(content)
    if not dept_info["department"] and not dept_info["major"]:
        dept_info = extract_department_and_major(title)
    
    if dept_info["department"]:
        metadata["department"] = dept_info["department"]
    if dept_info["major"]:
        metadata["major"] = dept_info["major"]
    
    # PhÃ¡t hiá»‡n lÄ©nh vá»±c
    field = detect_field(title, content)
    metadata["field"] = field
    
    # TrÃ­ch xuáº¥t cÃ¡c thÃ´ng tin khÃ¡c
    # Äá»‹a Ä‘iá»ƒm/cÆ¡ sá»Ÿ
    location_match = re.search(r'táº¡i\s+([^,.;:]+)', content, re.IGNORECASE)
    if location_match:
        metadata["location"] = location_match.group(1).strip()
    
    # Háº¡n chÃ³t/deadline
    deadline_match = re.search(r'háº¡n\s+([^,.;:]+)', content, re.IGNORECASE)
    if deadline_match:
        metadata["deadline"] = deadline_match.group(1).strip()
    
    return metadata

def generate_chunk_title(original_title, metadata, chunk_content, chunk_index):
    """Táº¡o tiÃªu Ä‘á» mÃ´ táº£ cho chunk."""
    title_parts = []
    
    # ThÃªm lÄ©nh vá»±c
    field_mapping = {
        "tuyen_sinh": "Tuyá»ƒn sinh",
        "ket_qua": "Káº¿t quáº£ xÃ©t tuyá»ƒn",
        "nganh": "ThÃ´ng tin ngÃ nh",
        "khac": "ThÃ´ng tin"
    }
    
    if "field" in metadata:
        title_parts.append(field_mapping.get(metadata["field"], "ThÃ´ng tin"))
    
    # ThÃªm nÄƒm há»c
    if "year" in metadata:
        title_parts.append(metadata["year"])
    
    # ThÃªm ngÃ nh/khoa
    if "major" in metadata:
        title_parts.append(metadata["major"])
    elif "department" in metadata:
        title_parts.append(metadata["department"])
    
    # ThÃªm pháº§n tá»« tiÃªu Ä‘á» gá»‘c náº¿u tiÃªu Ä‘á» táº¡o ra quÃ¡ ngáº¯n
    if len(" ".join(title_parts)) < 20:
        short_original = original_title[:50].strip()
        if short_original not in " ".join(title_parts):
            title_parts.append(f"- {short_original}")
    
    # ThÃªm sá»‘ chunk náº¿u cÃ³ nhiá»u chunk
    if chunk_index > 0:
        title_parts.append(f"(Pháº§n {chunk_index+1})")
    
    return " ".join(title_parts)

def create_chunk_id(source_file, content, chunk_index):
    """Táº¡o ID duy nháº¥t cho má»—i chunk."""
    source_name = Path(source_file).stem
    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
    return f"{source_name}_{content_hash}_{chunk_index}"

def process_markdown_file(folder):
    """Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file markdown trong thÆ° má»¥c."""
    folder_path = Path(folder)
    output_path = Path(folder_dir)
    print(f"ğŸ“ Äang xá»­ lÃ½ thÆ° má»¥c: {folder_path}")
    print(f"ğŸ“‚ ThÆ° má»¥c Ä‘áº§u ra: {output_path}")
    
    # Táº¡o thÆ° má»¥c Ä‘áº§u ra náº¿u chÆ°a tá»“n táº¡i
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Láº¥y danh sÃ¡ch táº¥t cáº£ cÃ¡c file markdown
    files = list(folder_path.glob("*.md"))
    all_chunks = []
    
    for file in tqdm(files, desc="ğŸ“„ Äang xá»­ lÃ½ cÃ¡c file"):
        try:
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # TÃ¡ch tiÃªu Ä‘á» vÃ  ná»™i dung
            lines = content.splitlines()
            title_line = ""
            content_start = 0
            
            # TÃ¬m dÃ²ng tiÃªu Ä‘á» (báº¯t Ä‘áº§u báº±ng #)
            for i, line in enumerate(lines):
                if line.strip().startswith("#"):
                    title_line = line.replace("#", "").strip()
                    content_start = i + 1
                    break
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á», láº¥y dÃ²ng Ä‘áº§u tiÃªn
            if not title_line and lines:
                title_line = lines[0].strip()
                content_start = 1
            
            # Láº¥y ná»™i dung chÃ­nh
            main_content = "\n".join(lines[content_start:])
            
            # TrÃ­ch xuáº¥t metadata
            metadata = extract_metadata(main_content, title_line)
            
            # Chia ná»™i dung thÃ nh cÃ¡c pháº§n dá»±a trÃªn tiÃªu Ä‘á» cáº¥p 2
            sections = []
            current_section = {"header": "", "content": ""}
            
            for line in lines[content_start:]:
                if line.strip().startswith("##"):
                    # LÆ°u pháº§n trÆ°á»›c Ä‘Ã³ náº¿u cÃ³
                    if current_section["content"].strip():
                        sections.append(current_section)
                    
                    # Báº¯t Ä‘áº§u pháº§n má»›i
                    current_section = {
                        "header": line.replace("#", "").strip(),
                        "content": ""
                    }
                else:
                    current_section["content"] += line + "\n"
            
            # ThÃªm pháº§n cuá»‘i cÃ¹ng
            if current_section["content"].strip():
                sections.append(current_section)
            
            # Náº¿u khÃ´ng cÃ³ pháº§n nÃ o, táº¡o má»™t pháº§n máº·c Ä‘á»‹nh
            if not sections:
                sections = [{"header": "", "content": main_content}]
            
            # Xá»­ lÃ½ tá»«ng pháº§n vÃ  táº¡o chunks
            file_chunks = []
            
            for i, section in enumerate(sections):
                section_content = section["content"].strip()
                if not section_content:
                    continue
                
                # ThÃªm header vÃ o ná»™i dung náº¿u cÃ³
                if section["header"]:
                    section_content = f"{section['header']}\n\n{section_content}"
                
                # Chia pháº§n thÃ nh cÃ¡c chunks ngá»¯ nghÄ©a
                content_chunks = chunk_content_by_semantics(section_content)
                
                for j, chunk_text in enumerate(content_chunks):
                    chunk_id = create_chunk_id(str(file), chunk_text, j)
                    chunk_title = generate_chunk_title(title_line, metadata, chunk_text, j if len(content_chunks) > 1 else -1)
                    
                    chunk_data = {
                        "id": chunk_id,
                        "title": chunk_title,
                        "content": chunk_text.strip(),
                        "source_file": str(file.name),
                        "source_url": "", # Sáº½ Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« ná»™i dung náº¿u cÃ³
                        "metadata": metadata.copy()
                    }
                    
                    # TrÃ­ch xuáº¥t URL nguá»“n náº¿u cÃ³
                    url_match = re.search(r'_Nguá»“n:\s*\[(.*?)\]\((.*?)\)_', chunk_text)
                    if url_match:
                        chunk_data["source_url"] = url_match.group(2)
                    
                    # ThÃªm metadata bá»• sung
                    chunk_data["metadata"]["chunk_index"] = j
                    chunk_data["metadata"]["section_index"] = i
                    chunk_data["metadata"]["token_count"] = len(chunk_text.split())
                    chunk_data["metadata"]["original_title"] = title_line
                    if section["header"]:
                        chunk_data["metadata"]["section_title"] = section["header"]
                    
                    file_chunks.append(chunk_data)
                    all_chunks.append(chunk_data)
            
            # LÆ°u chunks cá»§a file vÃ o JSON riÃªng
            json_filename = output_path / (file.stem + ".json")
            with open(json_filename, 'w', encoding='utf-8') as out_file:
                json.dump(file_chunks, out_file, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ file {file}: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # LÆ°u táº¥t cáº£ chunks vÃ o má»™t file tá»•ng há»£p
    all_chunks_file = output_path / "all_chunks.json"
    with open(all_chunks_file, 'w', encoding='utf-8') as out_file:
        json.dump(all_chunks, out_file, ensure_ascii=False, indent=2)
    
    # Táº¡o file thá»‘ng kÃª
    stats = {
        "total_files": len(files),
        "total_chunks": len(all_chunks),
        "fields": {},
        "years": {},
        "departments": {},
        "majors": {},
        "processed_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # TÃ­nh toÃ¡n thá»‘ng kÃª
    for chunk in all_chunks:
        field = chunk["metadata"].get("field", "unknown")
        year = chunk["metadata"].get("year", "unknown")
        department = chunk["metadata"].get("department", "unknown")
        major = chunk["metadata"].get("major", "unknown")
        
        stats["fields"][field] = stats["fields"].get(field, 0) + 1
        stats["years"][year] = stats["years"].get(year, 0) + 1
        stats["departments"][department] = stats["departments"].get(department, 0) + 1
        stats["majors"][major] = stats["majors"].get(major, 0) + 1
    
    # LÆ°u thá»‘ng kÃª
    stats_file = output_path / "stats.json"
    with open(stats_file, 'w', encoding='utf-8') as out_file:
        json.dump(stats, out_file, ensure_ascii=False, indent=2)
    
    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {len(files)} files thÃ nh {len(all_chunks)} chunks.")
    print(f"ğŸ“Š Thá»‘ng kÃª Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o {stats_file}")

# Cháº¡y xá»­ lÃ½
if __name__ == "__main__":
    process_markdown_file(data_dir)