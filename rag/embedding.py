import os
import json
from pathlib import Path
from typing import List, Dict
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, OptimizersConfigDiff
from tqdm import tqdm
import re
import uuid
from dotenv import load_dotenv

# Thi·∫øt l·∫≠p th∆∞ m·ª•c hi·ªán t·∫°i
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# C·∫•u h√¨nh
CHUNKED_DATA_DIR = "chunked_json"  # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë√£ ph√¢n ƒëo·∫°n
EMBEDDING_MODEL_NAME = "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"  # M√¥ h√¨nh ƒëa ng√¥n ng·ªØ h·ªó tr·ª£ ti·∫øng Vi·ªát
BATCH_SIZE = 8
MAX_LENGTH = 256
TITLE_WEIGHT = 10.0  # TƒÉng tr·ªçng s·ªë c·ªßa ti√™u ƒë·ªÅ
COLLECTION_NAME = "uit_documents_semantic"

# Kh·ªüi t·∫°o Qdrant client
qdrant_client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY"),
    timeout=60.0  # ho·∫∑c l·ªõn h∆°n n·∫øu c·∫ßn, v√≠ d·ª• 120.0
)

# Kh·ªüi t·∫°o model v√† tokenizer
print(f"üîß ƒêang t·∫£i m√¥ h√¨nh embedding: {EMBEDDING_MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üíª S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
model.to(device)
model.eval()


def clean_text(text: str) -> str:
    """L√†m s·∫°ch vƒÉn b·∫£n nh∆∞ng gi·ªØ l·∫°i c·∫•u tr√∫c metadata"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def get_embeddings(texts: List[str]) -> np.ndarray:
    """T·∫°o embeddings cho danh s√°ch vƒÉn b·∫£n"""
    try:
        # ƒê·∫£m b·∫£o vƒÉn b·∫£n kh√¥ng tr·ªëng
        valid_texts = [text if text.strip() else "empty content" for text in texts]
        
        inputs = tokenizer(
            valid_texts,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt",
            return_attention_mask=True
        ).to(device)

        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ID token v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc vocabulary
        if torch.any(inputs["input_ids"] >= tokenizer.vocab_size):
            inputs["input_ids"] = torch.clamp(inputs["input_ids"], max=tokenizer.vocab_size - 1)

        with torch.no_grad():
            outputs = model(**inputs)

        # S·ª≠ d·ª•ng [CLS] token embedding (vector ƒë·∫°i di·ªán cho to√†n b·ªô vƒÉn b·∫£n)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        return embeddings
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o embeddings: {e}")
        # Tr·∫£ v·ªÅ vector 0 v·ªõi k√≠ch th∆∞·ªõc ph√π h·ª£p n·∫øu c√≥ l·ªói
        return np.zeros((len(texts), model.config.hidden_size))


def combine_embeddings(title_embedding: np.ndarray, content_embedding: np.ndarray, title_weight: float = 1.0) -> np.ndarray:
    """
    K·∫øt h·ª£p embedding c·ªßa ti√™u ƒë·ªÅ v√† n·ªôi dung b·∫±ng c√°ch c·ªông c√≥ tr·ªçng s·ªë.

    Args:
        title_embedding: Embedding c·ªßa ti√™u ƒë·ªÅ.
        content_embedding: Embedding c·ªßa n·ªôi dung.
        title_weight: Tr·ªçng s·ªë c·ªßa ti√™u ƒë·ªÅ.

    Returns:
        Embedding k·∫øt h·ª£p ƒë√£ chu·∫©n h√≥a.
    """
    if content_embedding.ndim == 1:
        combined = (title_embedding * title_weight + content_embedding) / (title_weight + 1)
    else:
        combined = (title_embedding * title_weight + np.mean(content_embedding, axis=0)) / (title_weight + 1)
    
    # Chu·∫©n h√≥a vector ƒë·ªÉ ƒë·∫£m b·∫£o kho·∫£ng c√°ch cosine ho·∫°t ƒë·ªông t·ªët
    norm = np.linalg.norm(combined)
    if norm > 0:
        combined = combined / norm
    
    return combined


def process_json_file(json_file_path: str) -> List[PointStruct]:
    """
    X·ª≠ l√Ω m·ªôt file JSON ch·ª©a c√°c ƒëo·∫°n ƒë√£ ph√¢n v√† t·∫°o points cho Qdrant.
    M·ªói ƒëo·∫°n ƒë∆∞·ª£c t·∫°o th√†nh m·ªôt point ri√™ng nh∆∞ng v·∫´n gi·ªØ meta-data ƒë·∫ßy ƒë·ªß t·ª´ chunking.
    """
    try:
        with open(json_file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not data:
            print(f"‚ö†Ô∏è File tr·ªëng: {json_file_path}")
            return []

        points = []

        # X·ª≠ l√Ω t·ª´ng ƒëo·∫°n (chunk) th√†nh m·ªôt point ri√™ng
        for chunk in data:
            title = chunk.get("title", "")
            content = chunk.get("content", "")

            if not content.strip():
                continue

            # L·∫•y metadata n·∫øu c√≥
            metadata = chunk.get("metadata", {})
            field = metadata.get("field", "")
            year = metadata.get("year", "")
            department = metadata.get("department", "chung")
            major = metadata.get("major", "")
            chunk_id = chunk.get("id", "") or metadata.get("chunk_id", "")
            source_file = chunk.get("source_file", "")
            section_title = metadata.get("section_title", "")
            original_title = metadata.get("original_title", "") or chunk.get("original_title", "")
            section_index = metadata.get("section_index", None)
            chunk_index = metadata.get("chunk_index", None)
            token_count = metadata.get("token_count", None)
            source_url = chunk.get("source_url", "")

            # T·∫°o embedding cho ti√™u ƒë·ªÅ v√† n·ªôi dung
            clean_title = clean_text(title)
            clean_content = clean_text(content)

            title_embedding = get_embeddings([clean_title])[0]  # Vector c·ªßa ti√™u ƒë·ªÅ
            content_embedding = get_embeddings([clean_content])[0]  # Vector c·ªßa n·ªôi dung

            # K·∫øt h·ª£p hai embedding v·ªõi tr·ªçng s·ªë
            combined_embedding = combine_embeddings(title_embedding, content_embedding, TITLE_WEIGHT)

            # T·∫°o ID h·ª£p l·ªá cho Qdrant
            point_id = str(uuid.uuid4())

            # T·∫°o point cho Qdrant, l∆∞u ƒë·∫ßy ƒë·ªß metadata
            payload = {
                "title": title,
                "content": content,
                "source_file": str(source_file),
                "chunk_id": chunk_id,
                "field": field,
                "year": year,
                "department": department,
                "major": major,
                "original_title": original_title,
                "section_title": section_title,
                "section_index": section_index,
                "chunk_index": chunk_index,
                "token_count": token_count,
                "source_url": source_url
            }
            # X√≥a c√°c tr∆∞·ªùng None ƒë·ªÉ tr√°nh l·ªói Qdrant
            payload = {k: v for k, v in payload.items() if v is not None}

            point = PointStruct(
                id=point_id,
                vector=combined_embedding.tolist(),
                payload=payload
            )

            points.append(point)

        return points

    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω file {json_file_path}: {e}")
        return []

def process_in_batches(all_points: List[PointStruct], batch_size: int = 50):
    """X·ª≠ l√Ω v√† t·∫£i l√™n c√°c points theo batch"""
    total_batches = (len(all_points) + batch_size - 1) // batch_size
    
    for i in tqdm(range(total_batches), desc="üì§ ƒêang t·∫£i l√™n Qdrant"):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(all_points))
        batch_points = all_points[start_idx:end_idx]
        
        try:
            # Upload points v·ªõi wait=True ƒë·ªÉ ƒë·∫£m b·∫£o indexing ho√†n th√†nh
            qdrant_client.upsert(
                collection_name=COLLECTION_NAME,
                points=batch_points,
                wait=True  # ƒê·ª£i cho ƒë·∫øn khi indexing ho√†n th√†nh
            )
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i l√™n batch {i+1}/{total_batches}: {e}")
            # Th·ª≠ l·∫°i v·ªõi batch size nh·ªè h∆°n n·∫øu c√≥ l·ªói
            if len(batch_points) > 1:
                print(f"üîÑ Th·ª≠ l·∫°i v·ªõi batch size nh·ªè h∆°n...")
                for point in batch_points:
                    try:
                        qdrant_client.upsert(
                            collection_name=COLLECTION_NAME,
                            points=[point],
                            wait=True
                        )
                    except Exception as inner_e:
                        print(f"‚ùå L·ªói khi t·∫£i l√™n point ƒë∆°n l·∫ª: {inner_e}")
        
        # In th√¥ng tin ti·∫øn ƒë·ªô indexing sau m·ªói batch
        if (i + 1) % 10 == 0 or i == total_batches - 1:
            try:
                collection_info = qdrant_client.get_collection(COLLECTION_NAME)
                print(f"üìä Collection info - Points: {collection_info.points_count}, "
                      f"Indexed vectors: {collection_info.indexed_vectors_count}")
            except:
                pass


def create_collection(collection_name: str, vector_size: int = 768):
    """T·∫°o collection m·ªõi trong Qdrant"""
    try:
        # Ki·ªÉm tra xem collection ƒë√£ t·ªìn t·∫°i ch∆∞a
        collections = qdrant_client.get_collections()
        collection_names = [c.name for c in collections.collections]
        
        if collection_name in collection_names:
            confirm = input(f"‚ö†Ô∏è Collection '{collection_name}' ƒë√£ t·ªìn t·∫°i. B·∫°n c√≥ mu·ªën x√≥a v√† t·∫°o l·∫°i kh√¥ng? (y/n): ")
            if confirm.lower() == 'y':
                qdrant_client.delete_collection(collection_name=collection_name)
            else:
                print(f"‚úÖ Ti·∫øp t·ª•c s·ª≠ d·ª•ng collection: {collection_name}")
                # C·∫≠p nh·∫≠t c·∫•u h√¨nh optimizer cho collection hi·ªán t·∫°i
                try:
                    qdrant_client.update_collection(
                        collection_name=collection_name,
                        optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
                    )
                    print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t indexing_threshold=0 cho collection: {collection_name}")
                except Exception as opt_e:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t optimizer config: {opt_e}")
                return
        
        print(f"üèóÔ∏è ƒêang t·∫°o collection m·ªõi: {collection_name}")
        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            ),
            # Thi·∫øt l·∫≠p indexing_threshold=0 ngay khi t·∫°o collection
            optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
        )
        print(f"‚úÖ ƒê√£ t·∫°o collection v·ªõi indexing_threshold=0: {collection_name}")
    
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o collection: {e}")
        # N·∫øu l·ªói v·ªõi optimizer_config, th·ª≠ t·∫°o collection th√¥ng th∆∞·ªùng
        try:
            print(f"üîÑ Th·ª≠ t·∫°o collection kh√¥ng c√≥ optimizer_config...")
            qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )
            # Sau ƒë√≥ c·∫≠p nh·∫≠t optimizer_config
            qdrant_client.update_collection(
                collection_name=collection_name,
                optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
            )
            print(f"‚úÖ ƒê√£ t·∫°o collection v√† c·∫≠p nh·∫≠t indexing_threshold=0: {collection_name}")
        except Exception as e2:
            print(f"‚ùå L·ªói khi t·∫°o collection (l·∫ßn 2): {e2}")


def process_all_files(data_dir: str = CHUNKED_DATA_DIR, collection_name: str = COLLECTION_NAME):
    """X·ª≠ l√Ω t·∫•t c·∫£ c√°c file JSON trong th∆∞ m·ª•c v√† t·∫£i l√™n Qdrant"""
    # T·∫°o collection
    create_collection(collection_name)
    
    # Danh s√°ch t·∫•t c·∫£ c√°c files JSON
    json_files = list(Path(data_dir).glob("*.json"))
    
    if not json_files:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file JSON n√†o trong th∆∞ m·ª•c: {data_dir}")
        return
    
    print(f"üîç T√¨m th·∫•y {len(json_files)} file JSON ƒë·ªÉ x·ª≠ l√Ω")
    
    # X·ª≠ l√Ω t·ª´ng file v√† thu th·∫≠p points
    all_points = []
    chunk_count = 0
    
    for json_file in tqdm(json_files, desc="üìÑ ƒêang x·ª≠ l√Ω c√°c file"):
        points = process_json_file(json_file)
        all_points.extend(points)
        chunk_count += len(points)
        
        # Khi s·ªë l∆∞·ª£ng points ƒë·ªß l·ªõn, t·∫£i l√™n Qdrant v√† x√≥a b·ªô nh·ªõ
        if len(all_points) >= 1000:
            process_in_batches(all_points)
            print(f"‚úÖ ƒê√£ t·∫£i l√™n {len(all_points)} points")
            all_points = []
    
    # T·∫£i l√™n c√°c points c√≤n l·∫°i
    if all_points:
        process_in_batches(all_points)
    
    print(f"üéâ Ho√†n th√†nh! T·ªïng c·ªông {chunk_count} ƒëo·∫°n ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n collection '{collection_name}'")


if __name__ == "__main__":
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh embedding v√† t·∫£i l√™n Qdrant")
    process_all_files()