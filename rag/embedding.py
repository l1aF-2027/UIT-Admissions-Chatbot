import os
import json
from pathlib import Path
from typing import List, Dict
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, OptimizersConfigDiff
from tqdm import tqdm
import re
import uuid
from dotenv import load_dotenv

# Thiáº¿t láº­p thÆ° má»¥c hiá»‡n táº¡i
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)

# Táº£i biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Cáº¥u hÃ¬nh
CHUNKED_DATA_DIR = "chunked_json"  # ThÆ° má»¥c chá»©a dá»¯ liá»‡u Ä‘Ã£ phÃ¢n Ä‘oáº¡n
EMBEDDING_MODEL_NAME = "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"  # MÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ há»— trá»£ tiáº¿ng Viá»‡t
BATCH_SIZE = 8
MAX_LENGTH = 256
TITLE_WEIGHT = 10.0  # TÄƒng trá»ng sá»‘ cá»§a tiÃªu Ä‘á»
COLLECTION_NAME = "uit_documents_semantic"

# Khá»Ÿi táº¡o Qdrant client
qdrant_client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY"),
    timeout=60.0  # hoáº·c lá»›n hÆ¡n náº¿u cáº§n, vÃ­ dá»¥ 120.0
)

# Khá»Ÿi táº¡o model vÃ  tokenizer
print(f"ğŸ”§ Äang táº£i mÃ´ hÃ¬nh embedding: {EMBEDDING_MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ’» Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")
model.to(device)
model.eval()


def clean_text(text: str) -> str:
    """LÃ m sáº¡ch vÄƒn báº£n nhÆ°ng giá»¯ láº¡i cáº¥u trÃºc metadata"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def get_embeddings(texts: List[str]) -> np.ndarray:
    """Táº¡o embeddings cho danh sÃ¡ch vÄƒn báº£n"""
    try:
        # Äáº£m báº£o vÄƒn báº£n khÃ´ng trá»‘ng
        valid_texts = [text if text.strip() else "empty content" for text in texts]
        
        inputs = tokenizer(
            valid_texts,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt",
            return_attention_mask=True
        ).to(device)

        # Xá»­ lÃ½ trÆ°á»ng há»£p ID token vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c vocabulary
        if torch.any(inputs["input_ids"] >= tokenizer.vocab_size):
            inputs["input_ids"] = torch.clamp(inputs["input_ids"], max=tokenizer.vocab_size - 1)

        with torch.no_grad():
            outputs = model(**inputs)

        # Sá»­ dá»¥ng [CLS] token embedding (vector Ä‘áº¡i diá»‡n cho toÃ n bá»™ vÄƒn báº£n)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        return embeddings
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o embeddings: {e}")
        # Tráº£ vá» vector 0 vá»›i kÃ­ch thÆ°á»›c phÃ¹ há»£p náº¿u cÃ³ lá»—i
        return np.zeros((len(texts), model.config.hidden_size))


def combine_embeddings(title_embedding: np.ndarray, content_embedding: np.ndarray, title_weight: float = 1.0) -> np.ndarray:
    """
    Káº¿t há»£p embedding cá»§a tiÃªu Ä‘á» vÃ  ná»™i dung báº±ng cÃ¡ch cá»™ng cÃ³ trá»ng sá»‘.

    Args:
        title_embedding: Embedding cá»§a tiÃªu Ä‘á».
        content_embedding: Embedding cá»§a ná»™i dung.
        title_weight: Trá»ng sá»‘ cá»§a tiÃªu Ä‘á».

    Returns:
        Embedding káº¿t há»£p Ä‘Ã£ chuáº©n hÃ³a.
    """
    if content_embedding.ndim == 1:
        combined = (title_embedding * title_weight + content_embedding) / (title_weight + 1)
    else:
        combined = (title_embedding * title_weight + np.mean(content_embedding, axis=0)) / (title_weight + 1)
    
    # Chuáº©n hÃ³a vector Ä‘á»ƒ Ä‘áº£m báº£o khoáº£ng cÃ¡ch cosine hoáº¡t Ä‘á»™ng tá»‘t
    norm = np.linalg.norm(combined)
    if norm > 0:
        combined = combined / norm
    
    return combined


def process_json_file(json_file_path: str) -> List[PointStruct]:
    """
    Xá»­ lÃ½ má»™t file JSON chá»©a cÃ¡c Ä‘oáº¡n Ä‘Ã£ phÃ¢n vÃ  táº¡o points cho Qdrant.
    Má»—i Ä‘oáº¡n Ä‘Æ°á»£c táº¡o thÃ nh má»™t point riÃªng nhÆ°ng váº«n giá»¯ meta-data Ä‘áº§y Ä‘á»§ tá»« chunking.
    """
    try:
        with open(json_file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not data:
            print(f"âš ï¸ File trá»‘ng: {json_file_path}")
            return []

        points = []

        # Xá»­ lÃ½ tá»«ng Ä‘oáº¡n (chunk) thÃ nh má»™t point riÃªng
        for chunk in data:
            title = chunk.get("title", "")
            content = chunk.get("content", "")

            if not content.strip():
                continue

            # Láº¥y metadata náº¿u cÃ³
            metadata = chunk.get("metadata", {})
            field = metadata.get("field", "")
            year = metadata.get("year", "")
            department = metadata.get("department", "chung")
            major = metadata.get("major", "")
            chunk_id = chunk.get("id", "") or metadata.get("chunk_id", "")
            source_file = chunk.get("source_file", "")
            section_title = metadata.get("section_title", "")
            original_title = metadata.get("original_title", "") or chunk.get("original_title", "")
            section_index = metadata.get("section_index", None)
            chunk_index = metadata.get("chunk_index", None)
            token_count = metadata.get("token_count", None)
            source_url = chunk.get("source_url", "")

            # Táº¡o embedding cho tiÃªu Ä‘á» vÃ  ná»™i dung
            clean_title = clean_text(title)
            clean_content = clean_text(content)

            title_embedding = get_embeddings([clean_title])[0]  # Vector cá»§a tiÃªu Ä‘á»
            content_embedding = get_embeddings([clean_content])[0]  # Vector cá»§a ná»™i dung

            # Káº¿t há»£p hai embedding vá»›i trá»ng sá»‘
            combined_embedding = combine_embeddings(title_embedding, content_embedding, TITLE_WEIGHT)

            # Táº¡o ID há»£p lá»‡ cho Qdrant
            point_id = str(uuid.uuid4())

            # Táº¡o point cho Qdrant, lÆ°u Ä‘áº§y Ä‘á»§ metadata
            payload = {
                "title": title,
                "content": content,
                "source_file": str(source_file),
                "chunk_id": chunk_id,
                "field": field,
                "year": year,
                "department": department,
                "major": major,
                "original_title": original_title,
                "section_title": section_title,
                "section_index": section_index,
                "chunk_index": chunk_index,
                "token_count": token_count,
                "source_url": source_url
            }
            # XÃ³a cÃ¡c trÆ°á»ng None Ä‘á»ƒ trÃ¡nh lá»—i Qdrant
            payload = {k: v for k, v in payload.items() if v is not None}

            point = PointStruct(
                id=point_id,
                vector=combined_embedding.tolist(),
                payload=payload
            )

            points.append(point)

        return points

    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ file {json_file_path}: {e}")
        return []

def process_in_batches(all_points: List[PointStruct], batch_size: int = 50):
    """Xá»­ lÃ½ vÃ  táº£i lÃªn cÃ¡c points theo batch"""
    total_batches = (len(all_points) + batch_size - 1) // batch_size
    
    for i in tqdm(range(total_batches), desc="ğŸ“¤ Äang táº£i lÃªn Qdrant"):
        start_idx = i * batch_size
        end_idx = min(start_idx + batch_size, len(all_points))
        batch_points = all_points[start_idx:end_idx]
        
        try:
            # Upload points vá»›i wait=True Ä‘á»ƒ Ä‘áº£m báº£o indexing hoÃ n thÃ nh
            qdrant_client.upsert(
                collection_name=COLLECTION_NAME,
                points=batch_points,
                wait=True  # Äá»£i cho Ä‘áº¿n khi indexing hoÃ n thÃ nh
            )
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i lÃªn batch {i+1}/{total_batches}: {e}")
            # Thá»­ láº¡i vá»›i batch size nhá» hÆ¡n náº¿u cÃ³ lá»—i
            if len(batch_points) > 1:
                print(f"ğŸ”„ Thá»­ láº¡i vá»›i batch size nhá» hÆ¡n...")
                for point in batch_points:
                    try:
                        qdrant_client.upsert(
                            collection_name=COLLECTION_NAME,
                            points=[point],
                            wait=True
                        )
                    except Exception as inner_e:
                        print(f"âŒ Lá»—i khi táº£i lÃªn point Ä‘Æ¡n láº»: {inner_e}")
        
        # In thÃ´ng tin tiáº¿n Ä‘á»™ indexing sau má»—i batch
        if (i + 1) % 10 == 0 or i == total_batches - 1:
            try:
                collection_info = qdrant_client.get_collection(COLLECTION_NAME)
                print(f"ğŸ“Š Collection info - Points: {collection_info.points_count}, "
                      f"Indexed vectors: {collection_info.indexed_vectors_count}")
            except:
                pass


def create_collection(collection_name: str, vector_size: int = 768):
    """Táº¡o collection má»›i trong Qdrant"""
    try:
        # Kiá»ƒm tra xem collection Ä‘Ã£ tá»“n táº¡i chÆ°a
        collections = qdrant_client.get_collections()
        collection_names = [c.name for c in collections.collections]
        
        if collection_name in collection_names:
            confirm = input(f"âš ï¸ Collection '{collection_name}' Ä‘Ã£ tá»“n táº¡i. Báº¡n cÃ³ muá»‘n xÃ³a vÃ  táº¡o láº¡i khÃ´ng? (y/n): ")
            if confirm.lower() == 'y':
                qdrant_client.delete_collection(collection_name=collection_name)
            else:
                print(f"âœ… Tiáº¿p tá»¥c sá»­ dá»¥ng collection: {collection_name}")
                # Cáº­p nháº­t cáº¥u hÃ¬nh optimizer cho collection hiá»‡n táº¡i
                try:
                    qdrant_client.update_collection(
                        collection_name=collection_name,
                        optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
                    )
                    print(f"âœ… ÄÃ£ cáº­p nháº­t indexing_threshold=0 cho collection: {collection_name}")
                except Exception as opt_e:
                    print(f"âš ï¸ KhÃ´ng thá»ƒ cáº­p nháº­t optimizer config: {opt_e}")
                return
        
        print(f"ğŸ—ï¸ Äang táº¡o collection má»›i: {collection_name}")
        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            ),
            # Thiáº¿t láº­p indexing_threshold=0 ngay khi táº¡o collection
            optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
        )
        print(f"âœ… ÄÃ£ táº¡o collection vá»›i indexing_threshold=0: {collection_name}")
    
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o collection: {e}")
        # Náº¿u lá»—i vá»›i optimizer_config, thá»­ táº¡o collection thÃ´ng thÆ°á»ng
        try:
            print(f"ğŸ”„ Thá»­ táº¡o collection khÃ´ng cÃ³ optimizer_config...")
            qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )
            # Sau Ä‘Ã³ cáº­p nháº­t optimizer_config
            qdrant_client.update_collection(
                collection_name=collection_name,
                optimizer_config=OptimizersConfigDiff(indexing_threshold=0)
            )
            print(f"âœ… ÄÃ£ táº¡o collection vÃ  cáº­p nháº­t indexing_threshold=0: {collection_name}")
        except Exception as e2:
            print(f"âŒ Lá»—i khi táº¡o collection (láº§n 2): {e2}")


def process_all_files(data_dir: str = CHUNKED_DATA_DIR, collection_name: str = COLLECTION_NAME):
    """Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file JSON trong thÆ° má»¥c vÃ  táº£i lÃªn Qdrant"""
    # Táº¡o collection
    create_collection(collection_name)
    
    # Danh sÃ¡ch táº¥t cáº£ cÃ¡c files JSON
    json_files = list(Path(data_dir).glob("*.json"))
    
    if not json_files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON nÃ o trong thÆ° má»¥c: {data_dir}")
        return
    
    print(f"ğŸ” TÃ¬m tháº¥y {len(json_files)} file JSON Ä‘á»ƒ xá»­ lÃ½")
    
    # Xá»­ lÃ½ tá»«ng file vÃ  thu tháº­p points
    all_points = []
    chunk_count = 0
    
    for json_file in tqdm(json_files, desc="ğŸ“„ Äang xá»­ lÃ½ cÃ¡c file"):
        points = process_json_file(json_file)
        all_points.extend(points)
        chunk_count += len(points)
        
        # Khi sá»‘ lÆ°á»£ng points Ä‘á»§ lá»›n, táº£i lÃªn Qdrant vÃ  xÃ³a bá»™ nhá»›
        if len(all_points) >= 1000:
            process_in_batches(all_points)
            print(f"âœ… ÄÃ£ táº£i lÃªn {len(all_points)} points")
            all_points = []
    
    # Táº£i lÃªn cÃ¡c points cÃ²n láº¡i
    if all_points:
        process_in_batches(all_points)
    
    print(f"ğŸ‰ HoÃ n thÃ nh! Tá»•ng cá»™ng {chunk_count} Ä‘oáº¡n Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn collection '{collection_name}'")


if __name__ == "__main__":
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh embedding vÃ  táº£i lÃªn Qdrant")
    process_all_files()